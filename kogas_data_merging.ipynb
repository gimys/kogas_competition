{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "from   datetime import datetime, date\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 7)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the path of files\n",
    "\n",
    "filepath_mnth = []\n",
    "for file in glob.glob(\"//10.64.187.59/part1/[0-9]*\"):\n",
    "# for file in glob.glob(\"Z:/\"):\n",
    "    filepath_mnth.append(file)\n",
    "\n",
    "filepath_mnth_re = filepath_mnth[4:]+filepath_mnth[:3]\n",
    "filepath_mnth_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = []\n",
    "# for file in glob.glob(\"S:/2차펌프(1공장)/P301A*.csv\"):\n",
    "# for file in glob.glob(\"//10.64.187.59/part1/1월/2차펌프(1공장)/P3*.csv\"):\n",
    "for file in glob.glob(filepath_mnth_re[0]+\"/2차펌프(1공장)/*\"):\n",
    "# for file in glob.glob(\"Z:/\"):\n",
    "    filepath.append(file)\n",
    "\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collect the path of files by months respectively \n",
    "\n",
    "filedic = dict()\n",
    "filepath = []\n",
    "\n",
    "for i in range(len(filepath_mnth_re)):\n",
    "    for file in glob.glob(filepath_mnth_re[i]+\"/2차펌프(1공장)/*.csv\"):\n",
    "        filepath.append(file)\n",
    "    filedic[i] = filepath\n",
    "    filepath = []\n",
    "    print(i)\n",
    "\n",
    "## Define the name of pumps of 2nd factory\n",
    "\n",
    "pump_name_2nd_factory = []\n",
    "for r in list('ABCDEFGHIJKLMNOP'):\n",
    "    pump_name_2nd_factory.append('P301'+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make dictionary form\n",
    "\n",
    "pump_dic = dict()\n",
    "pump_path = []\n",
    "for j in range(len(pump_name_2nd_factory)):\n",
    "    for i in range(len(filedic)):\n",
    "        for r in filedic[i]:\n",
    "            if bool(re.search(pump_name_2nd_factory[j], r)):    \n",
    "                pump_path.append(r)\n",
    "            pass\n",
    "    pump_dic[j] = pump_path\n",
    "    pump_path = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Load\n",
    "use_cols = ['Time', 'Value']\n",
    "datasets = dict()\n",
    "grouped_datasets = dict()\n",
    "no = 0 # the length of pump_dic is 16\n",
    "filelist = pump_dic[no]\n",
    "for i in range(len(filelist)):\n",
    "    data = pd.read_csv(filelist[i], usecols = use_cols)\n",
    "    if len(data) != 0:\n",
    "#         data.columns = ['Time', filelist[i][13:-4]]\n",
    "        data.columns = ['Time', filelist[i][filelist[i].find('\\\\')+1:-4]]\n",
    "        data['Time'] = pd.to_datetime(data['Time'], format = \"%Y/%m/%d %H:%M:%S.%f\")\n",
    "        data['Time_conv'] = pd.to_datetime(data['Time'].map(lambda x: x.strftime('%Y/%m/%d %H:%M')))\n",
    "        datasets[i] = data\n",
    "        grouped_data = datasets[i].groupby('Time_conv', as_index=False).mean()\n",
    "        grouped_datasets[i] = grouped_data    \n",
    "        print(\"********** \", i+1, \"th dataset / total\", len(filelist), \"dataset is done **********\")\n",
    "    else: print(\"********** \", i+1, \"th dataset is empty **********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_index = dict()\n",
    "data_index = []\n",
    "pump_name = pump_name_2nd_factory[0]\n",
    "for j in range(len(features_dic[pump_name])):\n",
    "    for i in range(len(grouped_datasets)):\n",
    "        try:\n",
    "            data_index.append(bool(re.search('_'+features_dic[pump_name][j], str(grouped_datasets[i].columns))))\n",
    "        except KeyError:\n",
    "            data_index.append(False)\n",
    "    full_data_index[j] = data_index\n",
    "    data_index = []\n",
    "\n",
    "pump_features_dic = dict()\n",
    "foo = []\n",
    "for j in range(len(full_data_index)):\n",
    "    for index in list(np.where(full_data_index[j])[0]):\n",
    "        foo.append(grouped_datasets[index])\n",
    "    pump_features_dic[j] = pd.concat(foo)\n",
    "    foo = []\n",
    "    \n",
    "for merge_pump_name in pump_name_2nd_factory:\n",
    "    full_data_index = dict()\n",
    "    data_index = []\n",
    "    pump_name = merge_pump_name\n",
    "    for j in range(len(features_dic[pump_name])):\n",
    "        for i in range(len(grouped_datasets)):\n",
    "            try:\n",
    "                data_index.append(bool(re.search('_'+features_dic[pump_name][j], str(grouped_datasets[i].columns))))\n",
    "            except KeyError:\n",
    "                data_index.append(False)\n",
    "        full_data_index[j] = data_index\n",
    "        data_index = []\n",
    "    \n",
    "    pump_features_dic = dict()\n",
    "    foo = []\n",
    "    for j in range(len(full_data_index)):\n",
    "        for index in list(np.where(full_data_index[j])[0]):\n",
    "            foo.append(grouped_datasets[index])\n",
    "        pump_features_dic[j] = pd.concat(foo)\n",
    "        foo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make data market for merge(period = 1min)\n",
    "tmp_date = pd.date_range(start='12/31/2013', end='12/31/2014')\n",
    "tmp_df_date = pd.DataFrame(tmp_date, columns=['date'])\n",
    "tmp_df_date['date'] = tmp_df_date['date'].astype(str)\n",
    "tmp_df_date['date'] = tmp_df_date['date'].str[0:10]\n",
    "\n",
    "tmp_time = pd.date_range(start='00:00:00', end='23:59:00', freq='1min')\n",
    "tmp_df_time = pd.DataFrame(tmp_time, columns=['time'])\n",
    "tmp_df_time['time'] = tmp_df_time['time'].astype(str)\n",
    "tmp_df_time['time'] = tmp_df_time['time'].str[10:19]\n",
    "\n",
    "tmp_init_DT = pd.DataFrame(columns=['date', 'time'])\n",
    "tmp_init_DT2 = pd.DataFrame(columns=['date', 'time'])\n",
    "\n",
    "for i in range(0,366):\n",
    "    if i == 0:\n",
    "        tmp_init_DT['time'] = tmp_df_time['time']\n",
    "        tmp_init_DT['date'] = tmp_df_date['date'][0]\n",
    "    else:\n",
    "        tmp_init_DT2['time'] = tmp_df_time['time']\n",
    "        tmp_init_DT2['date'] = tmp_df_date['date'][i]\n",
    "        tmp_init_DT = tmp_init_DT.append(tmp_init_DT2)\n",
    "\n",
    "del tmp_df_time, tmp_df_date, tmp_init_DT2\n",
    "\n",
    "tmp_init_DT['Time_conv'] = tmp_init_DT['date']+tmp_init_DT['time']\n",
    "tmp_init_DT['Time_conv'] = pd.to_datetime(tmp_init_DT['Time_conv'], format = \"%Y/%m/%d %H:%M:%S.%f\")\n",
    "tmp_init_DT = tmp_init_DT.drop(columns= ['date', 'time'])\n",
    "tmp_init_DT = tmp_init_DT[900:-540]\n",
    "data_market = tmp_init_DT\n",
    "del tmp_init_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging(Left Join)\n",
    "\n",
    "merge_pump_name = pump_name_2nd_factory[0]\n",
    "merging_result = data_market\n",
    "for i in range(len(pump_features_dic)):\n",
    "    merging_result = pd.merge(merging_result, pump_features_dic[i], how = 'left', on = 'Time_conv')\n",
    "merging_result.to_csv('uniset_'+merge_pump_name+'.csv')\n",
    "print(merge_pump_name+'equipment file merge complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Merging Code\n",
    "## Only use this code for data load & merging\n",
    "for m in range(len(pump_name_2nd_factory)):\n",
    "    \n",
    "    use_cols = ['Time', 'Value']\n",
    "    datasets = dict()\n",
    "    grouped_datasets = dict()\n",
    "    no = m # the length of pump_dic is 16\n",
    "    filelist = pump_dic[no]\n",
    "    for i in range(len(filelist)):\n",
    "        data = pd.read_csv(filelist[i], usecols = use_cols)\n",
    "        if len(data) != 0:\n",
    "    #         data.columns = ['Time', filelist[i][13:-4]]\n",
    "            data.columns = ['Time', filelist[i][filelist[i].find('\\\\')+1:-4]]\n",
    "            data['Time'] = pd.to_datetime(data['Time'], format = \"%Y/%m/%d %H:%M:%S.%f\")\n",
    "            data['Time_conv'] = pd.to_datetime(data['Time'].map(lambda x: x.strftime('%Y/%m/%d %H:%M')))\n",
    "            datasets[i] = data\n",
    "            grouped_data = datasets[i].groupby('Time_conv', as_index=False).mean()\n",
    "            grouped_datasets[i] = grouped_data    \n",
    "            print(\"********** \", i+1, \"th dataset / total\", len(filelist), \"dataset is done **********\")\n",
    "        else: print(\"********** \", i+1, \"th dataset is empty **********\")\n",
    "    \n",
    "    \n",
    "    full_data_index = dict()\n",
    "    data_index = []\n",
    "    pump_name = pump_name_2nd_factory[m]\n",
    "    for j in range(len(features_dic[pump_name])):\n",
    "        for i in range(len(grouped_datasets)):\n",
    "            try:\n",
    "                data_index.append(bool(re.search('_'+features_dic[pump_name][j], str(grouped_datasets[i].columns))))\n",
    "            except KeyError:\n",
    "                data_index.append(False)\n",
    "        full_data_index[j] = data_index\n",
    "        data_index = []\n",
    "    \n",
    "    pump_features_dic = dict()\n",
    "    foo = []\n",
    "    for j in range(len(full_data_index)):\n",
    "        for index in list(np.where(full_data_index[j])[0]):\n",
    "            foo.append(grouped_datasets[index])\n",
    "        pump_features_dic[j] = pd.concat(foo)\n",
    "        foo = []\n",
    "        \n",
    "    merging_result = data_market\n",
    "    for k in range(len(pump_features_dic)):\n",
    "        merging_result = pd.merge(merging_result, pump_features_dic[k], how = 'left', on = 'Time_conv')\n",
    "    merging_result.to_csv('uniset_'+pump_name_2nd_factory[m]+'.csv')\n",
    "    print(pump_name_2nd_factory[m]+'장비의 merge file 제작 완료')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
